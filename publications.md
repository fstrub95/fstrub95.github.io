---
layout: page
title: Publications
permalink: /publications/
---

If you need more information about some papers, the code, or if you need the slides, feel free to contact me!

 **F. Strub**, M. Seurin, E. Perez, H. de Vries, P. Preux, A. Courville, O. Pietquin<br/>
*Visual Reasoning with Multi-hop Feature Modulation* <br/>
In Proc. ECCV (2018) <br/>
[Paper](https://arxiv.org/abs/1709.07871) - [code](https://github.com/fstrub95/guesswhat) 

V. Dumoulin, E. Perez, N. Schucher, **F. Strub**, H. Vries, A. Courville, Y. Bengio
*Feature-wise transformations* <br/>
In Proc. Distill (2018) <br/>
[Website](https://distill.pub/2018/feature-wise-transformations/) 

E. Perez, **F. Strub**, H. Vries, V. Dumoulin, A. Courville <br/>
*FiLM: Visual Reasoning with a General Conditioning Layer.* <br/>
In Proc. AAAI - Oral (2018) <br/>
[Paper](https://arxiv.org/abs/1709.07871) - [code1](https://github.com/ethanjperez/film) - [code2](https://github.com/GuessWhatGame/clevr)

S. Brodeur, E. Perez, A. Anand, F. Golemo, L. Celotti, **F. Strub**, J.Rouat, H. Larochelle, A. Courville <br/>
*HoME: A household multimodal environment.* <br/>
In Visually-Grounded Interaction and Language Workshop, NIPS 2017 <br/>
[Paper](https://arxiv.org/abs/1711.11017) - [code](https://github.com/HoME-Platform/home-platform) - [website](https://home-platform.github.io/)

E. Perez, H. Vries, **F. Strub**, V. Dumoulin, A. Courville <br/>
*Learning Visual Reasoning Without Strong Priors.* <br/>
In ICML Speech and Language Processing Workshop (2017). <br/>
[Paper](https://arxiv.org/abs/1707.03017) - [code1](https://github.com/ethanjperez/film) - [code2](https://github.com/GuessWhatGame/clevr)

H. de Vries\* , **F. Strub**\* , J. Mary, H. Larochelle, O. Pietquin, A. Courville <br/>
*Modulating Early visual Processing by language.* <br/>
In Proc. of NIPS - Spotlight (2017). <br/>
[Paper](https://arxiv.org/abs/1707.00683) - [code](https://github.com/GuessWhatGame) 

**F. Strub**, H. de Vries, J. Mary, B. Piot, A. Courville, O. Pietquin <br/>
*End-to-end optimization of goal-driven and visually grounded dialogue systems.* <br/>
In Proc. of IJCAI  - Oral presentation (2017).<br/>
[Paper](https://arxiv.org/abs/1703.05423) - [code](https://github.com/GuessWhatGame/guesswhat) - [website](https://guesswhat.ai/)

H. de Vries, **F. Strub**, S. Chandar, O. Pietquin, H. Larochelle, A. Courville <br/>
*GuessWhat?! Visual object discovery through multi-modal dialogue.* <br/>
In Proc. of CVPR - Spotlight (2017). <br/>
[Paper](https://arxiv.org/abs/1611.08481) - [code](https://github.com/GuessWhatGame/guesswhat) - [website](https://guesswhat.ai/)

J. PÃ©rolat, **F. Strub**, B. Piot, O. Pietquin <br/>
*Learning Nash Equilibrium for General-Sum Markov Games from Batch Data.* <br/>
In Proc. of AISTAT (2017). <br/>
[Paper](https://arxiv.org/abs/1606.08718) - [code](https://github.com/fstrub95/nashnetwork)

**F. Strub**, J. Mary, R. Gaudel <br/>
*Hybrid Recommender System based on Autoencoders.* <br/>
In Proc. of Recsys workshop DLRS (2017) <br/>
[Paper](https://arxiv.org/abs/1606.07659) - [code](https://github.com/fstrub95/Autoencoders_cf) 

**F. Strub**, J. Mary, P. Preux <br/> 
*Collaborative Filtering with Stacked Denoising Autoencoders and Sparse Inputs.* <br/>
In NIPS Workshop on Machine Learning for eCommerce. (2015). <br/>
[Paper](https://hal.archives-ouvertes.fr/hal-01256422/document) - [code](https://github.com/fstrub95/Autoencoders_cf)

\* stands for equal contribution
